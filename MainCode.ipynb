{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa082bb-5233-47d4-b312-eb13525f75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1b25c4-7623-4dba-ae0d-57bc76fc6a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824bc737-0d7d-40c7-9727-028d7173d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                       COMMAND                  CREATED          STATUS          PORTS                                       NAMES\n",
      "ddb0ec129ed2   nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest   \"/opt/nvidia/nvidia_…\"   23 minutes ago   Up 23 minutes   0.0.0.0:9038->8000/tcp, :::9038->8000/tcp   gallant_kepler\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2f5986-7397-43c4-84a6-7dffc13cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NGC_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72790bf6-a798-42dc-b6fd-d392432261a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/gsh-hg2bg2/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "! echo -e \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fbf7d-f2a7-41a3-80ca-bd4f3f7aa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! docker pull nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dad1cf-d41b-4c89-babe-a6df9fe9bf38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! docker pull nvcr.io/nim/meta/llama-3.1-70b-instruct:1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742d919-6f29-488c-ae9f-2e36e15553d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! docker pull nvcr.io/nim/mistralai/mistral-7b-instruct-v0.3:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65082f-faf9-4f09-be03-4531d14024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker pull nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55ed76-70ae-4aac-8653-e9248c4fbc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker pull nvcr.io/nim/meta/codellama-34b-instruct:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8cace1d-70f6-4667-b44b-dfdbfb5d531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                           TAG       IMAGE ID       CREATED        SIZE\n",
      "nvcr.io/nim/meta/codellama-34b-instruct              latest    877e1e28551e   11 days ago    13.2GB\n",
      "nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct   1.2       14d098dfc033   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct   latest    14d098dfc033   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama-3.1-8b-instruct               1.2.2     8774343a9244   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama-3.1-70b-instruct              1.2       c194c5427b1b   3 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama3-8b-instruct                  1.0.0     3cb29b0d79e6   4 months ago   12.5GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e894fdf-c653-4f39-9276-4bfa78efac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsh-hg2bg2/.cache/nim\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"{home}/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd7b484-5bda-4db2-8749-1708ae0a8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "!chmod 777 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ad9871-b6f1-48b3-a3e6-595b5d941c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your have been alloted the available port: 9528\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import socket\n",
    "def find_available_port(start=9000, end=9999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d73d95-1867-4108-b1c4-d14976ba5c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddb0ec129ed2\n"
     ]
    }
   ],
   "source": [
    " !docker stop $(docker ps -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc0ff1-3efd-4d34-b9ee-a37abe6c9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -d --rm --gpus 1 --name=llm_nim --shm-size=16GB  -v $LOCAL_NIM_CACHE:/opt/nim/.cache  -u $(id -u) -p $CONTAINER_PORT:8000 nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.2\n",
    "! sleep 30 # let's wait for the conrtainer to get up completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10648284-6bfc-40fb-af77-c071bd18b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus device=all --name=llm_nim --shm-size=16GB  -v $LOCAL_NIM_CACHE:/opt/nim/.cache  -u $(id -u) -p $CONTAINER_PORT:8000 nvcr.io/nim/meta/codellama-34b-instruct:latest\n",
    "! sleep 30 # let's wait for the container to get up completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67934c97-d4d4-4dce-b46c-eee22073832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                       COMMAND                  CREATED              STATUS              PORTS                                       NAMES\n",
      "b2aca6e298f1   nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest   \"/opt/nvidia/nvidia_…\"   About a minute ago   Up About a minute   0.0.0.0:9528->8000/tcp, :::9528->8000/tcp   musing_jang\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d0ae3-6277-4dc9-9d14-35fe9acfe770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']), model=\"meta/llama-3.1-8b-instruct\", temperature=0.2, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9589e3d-0268-43a2-9732-e969a1879048",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CONTAINER_PORT2'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT2']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1bf0c-c1df-428c-b383-4690a9aeb31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all --name=llm_nim2 --shm-size=16GB  -v $LOCAL_NIM_CACHE:/opt/nim/.cache  -u $(id -u) -p $CONTAINER_PORT2:8000 nvcr.io/nim/meta/codellama-34b-instruct:latest\n",
    "! sleep 30 # let's wait for the container to get up completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e62df-bfa3-4b9f-b59d-9bbb9bf45df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"export NGC_API_KEY=\"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\"\n",
    "export LOCAL_NIM_CACHE=\"~/.cache/nim\"\n",
    "mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "docker run -it --rm \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "    -u $(id -u) \\\n",
    "    -p 8001:8000 \\\n",
    "    nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71bb91d3-14c6-4126-8b6f-8b6ab55f3244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beaf028acc98\n"
     ]
    }
   ],
   "source": [
    " !docker stop $(docker ps -q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b04c417d-5408-4209-b4f7-9ad502bb459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                           TAG       IMAGE ID       CREATED        SIZE\n",
      "nvcr.io/nim/meta/codellama-34b-instruct              latest    877e1e28551e   11 days ago    13.2GB\n",
      "nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct   1.2       14d098dfc033   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct   latest    14d098dfc033   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama-3.1-8b-instruct               1.2.2     8774343a9244   2 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama-3.1-70b-instruct              1.2       c194c5427b1b   3 weeks ago    13.2GB\n",
      "nvcr.io/nim/meta/llama3-8b-instruct                  1.0.0     3cb29b0d79e6   4 months ago   12.5GB\n"
     ]
    }
   ],
   "source": [
    "! docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10f84d1d-d28f-46df-badc-d24be340bfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                            COMMAND                  CREATED          STATUS          PORTS                                       NAMES\n",
      "bc608f95ea1d   nvcr.io/nim/meta/codellama-34b-instruct:latest   \"/opt/nvidia/nvidia_…\"   29 minutes ago   Up 29 minutes   0.0.0.0:9528->8000/tcp, :::9528->8000/tcp   vigilant_kapitsa\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebbdd966-1b96-4c9c-af10-c75a3b27c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']), model=\"codellama/codellama-34b-instruct\", temperature=0.2, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704729a-4812-4bf4-8738-232deca884fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT2']), model=\"mistral-nemo-12b-instruct\", temperature=0.2, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of India?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2335e09-aad7-4281-ba58-2a2a7c72ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X 'POST' \\\n",
    "    \"http://0.0.0.0:${CONTAINER_PORT2}/v1/completions\" \\\n",
    "    -H \"accept: application/json\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"model\": \"meta/llama3.1-8b-instruct\", \"prompt\": \"What is the capital of France?\", \"max_tokens\": 64}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a943fb-bbf6-4b67-855b-fa584eed1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "        \"https://docs.nvidia.com/cuda/\",\n",
    "        \"https://github.com/NVIDIA/cuda-samples\",\n",
    "        \"https://github.com/openhackathons-org/nways_accelerated_programming/blob/main/_basic/cuda/jupyter_notebook/nways_cuda.ipynb\"\n",
    "        \"https://github.com/openhackathons-org/nways_multi_gpu/tree/main\",\n",
    "        \"https://reference.wolfram.com/language/CUDALink/tutorial/Programming.html\",\n",
    "        \"https://docs.python.org/3/reference/datamodel.html\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c172e7f-276e-4b55-b587-9f6c9ed4650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Union\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14cf4772-c873-4485-a175-0a2d4b7f09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model,embedding_path: str = \"./embed\"):\n",
    "\n",
    "    embedding_path = \"./embed\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(\"Total documents:\",len(documents))\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    print(\"Total texts:\",len(texts))\n",
    "    index_docs(embeddings_model,url, text_splitter, texts, embedding_path,)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b193fb6-679b-4fb0-a7fc-aa2f314d2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac38959c-77d9-4649-94ac-9ac0be6ddb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def index_docs(embeddings_model, url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks and create embeddings for them.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_model: Model used for creating embeddings.\n",
    "        url: Source url for the documents.\n",
    "        splitter: Splitter used to split the documents.\n",
    "        documents: List of documents whose embeddings need to be created.\n",
    "        dest_embed_dir: Destination directory for embeddings.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_texts = splitter.split_text(document.page_content)\n",
    "        texts.extend(chunk_texts)\n",
    "        metadatas.extend([document.metadata] * len(chunk_texts))\n",
    "\n",
    "    if os.path.exists(dest_embed_dir):\n",
    "        docsearch = FAISS.load_local(\n",
    "            folder_path=dest_embed_dir, \n",
    "            embeddings=embeddings_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        docsearch.add_texts(texts, metadatas=metadatas)\n",
    "    else:\n",
    "        docsearch = FAISS.from_texts(texts, embedding=embeddings_model, metadatas=metadatas)\n",
    "\n",
    "    docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9b39023-5d74-4d81-a949-805f768acf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeddings to ./embed\n",
      "Total documents: 5\n",
      "Total texts: 380\n",
      "Generated embedding successfully\n",
      "CPU times: user 1.31 s, sys: 293 ms, total: 1.6 s\n",
      "Wall time: 6.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_embeddings(embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "296b72ea-4b30-4f08-bd08-71befa2ddfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Embed documents\n",
    "embedding_path = \"./embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117b5c-7ac6-4753-82e9-7cf85cabf8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']),\n",
    "                 model=\"meta/llama-3.1-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_prompt=QA_PROMPT\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad785778-c8d2-4c4b-a2c0-c37af96c088b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qa = ConversationalRetrievalChain(\\n    retriever=docsearch.as_retriever(),\\n    combine_docs_chain=doc_chain,\\n    memory=memory,\\n   #return_source_documents=True,\\n    question_generator=question_generator,  # applying question generator\\n)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT']),\n",
    "                 model=\"codellama/codellama-34b-instruct\", temperature=0.2, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\", max_token_limit=1200)\n",
    "\n",
    "question_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "#chat = ChatNVIDIA(base_url=\"http://0.0.0.0:{}/v1\".format(os.environ['CONTAINER_PORT2']),\n",
    "#                 model=\"mistralai/mistral-7b-instruct-v0.3\", temperature=0.2, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "doc_chain = load_qa_chain(llm , chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={'prompt': QA_PROMPT},\n",
    ")\n",
    "\n",
    "'''qa = ConversationalRetrievalChain(\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory,\n",
    "   #return_source_documents=True,\n",
    "    question_generator=question_generator,  # applying question generator\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd54156-80a8-410a-9970-2997e23bffcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f278d16a-2679-4c39-bec4-5f9ccbb08e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': ' The answer to the question is:\\n\\n* It is not a data race because the GPU is not accessing the memory while the cudaGraphExec_t is being destroyed.\\n\\nExplanation:\\n\\nThe question is asking about the behavior of CUDA when a cudaGraphExec_t is destroyed without being synchronized. The answer is that it is not a data race because the GPU is not accessing the memory while the cudaGraphExec_t is being destroyed. This is because the GPU is only accessing the memory when it is executing the graph, and the graph is not being executed when the cudaGraphExec_t is being destroyed. Therefore, there is no conflict between the CPU and GPU accessing the same memory.', 'context': 'weakref.finalize provides a straightforward way to register a cleanup function to be called when an object is garbage collected. Note del x doesnâ\\x80\\x99t directly call x.__del__() â\\x80\\x94 the former decrements the reference count for x by one, and the latter is only called when xâ\\x80\\x99s reference count reaches zero. CPython implementation detail: It is possible for a reference cycle to prevent the reference count of an object from going to zero. In this case, the cycle will be later detected and deleted\\nweakref.finalize provides a straightforward way to register a cleanup function to be called when an object is garbage collected. Note del x doesnâ\\x80\\x99t directly call x.__del__() â\\x80\\x94 the former decrements the reference count for x by one, and the latter is only called when xâ\\x80\\x99s reference count reaches zero. CPython implementation detail: It is possible for a reference cycle to prevent the reference count of an object from going to zero. In this case, the cycle will be later detected and deleted\\nweakref.finalize provides a straightforward way to register a cleanup function to be called when an object is garbage collected. Note del x doesnâ\\x80\\x99t directly call x.__del__() â\\x80\\x94 the former decrements the reference count for x by one, and the latter is only called when xâ\\x80\\x99s reference count reaches zero. CPython implementation detail: It is possible for a reference cycle to prevent the reference count of an object from going to zero. In this case, the cycle will be later detected and deleted\\nweakref.finalize provides a straightforward way to register a cleanup function to be called when an object is garbage collected. Note del x doesnâ\\x80\\x99t directly call x.__del__() â\\x80\\x94 the former decrements the reference count for x by one, and the latter is only called when xâ\\x80\\x99s reference count reaches zero. CPython implementation detail: It is possible for a reference cycle to prevent the reference count of an object from going to zero. In this case, the cycle will be later detected and deleted'}\n"
     ]
    }
   ],
   "source": [
    "query = \"How does CUDA manage reference to user objects when a cudaGraphExec_t is destroyed without being sychronized?\"\n",
    "#result = qa({\"question\": query})\n",
    "result = qa.invoke({\"question\": query})\n",
    "answer = result.get('answer', result.get('output', ''))\n",
    "context = \"\\n\".join([doc.page_content for doc in result['source_documents']])\n",
    "response = {\"answer\":answer, \"context\":context }\n",
    "#print (answer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ee5b4-4f14-4ead-b21b-0859d70b0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1526d85-18ae-4f21-b0a1-f1619ad2df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = qa.invoke({\"question\": query,\"max_tokens\":700})\n",
    "answer = result.get('answer', result.get('output', ''))\n",
    "answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335610d-b420-4abb-ad27-0e61efda161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([doc.page_content for doc in result['source_documents']])\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836d20d-ad69-46e0-925d-d082db98eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is CUDA?\"\n",
    "result = qa({\"question\": query})\n",
    "answer = result.get('answer', result.get('output', ''))\n",
    "#context = \"\\n\".join([doc.page_content for doc in result['source_documents']])\n",
    "#result = qa({\"question\": query})\n",
    "#print(result.get(\"answer\"))\n",
    "#out = llm.invoke(\"What is CUDA\")\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574385a-cfe9-4858-96d0-081ddadcae77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e44d54-39ea-4579-928e-20cd5a8d5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you write a gpu kernel to add the elements of two arrays and store the output in a third array?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731a1c1-8a83-4bed-ae91-1d9048c46c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"list you dataset topic?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cefa72-9b73-4cef-a0d7-e8b3f7017169",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"list your Concepts and techniques in your dataset ?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ab283-0dd2-480d-8c34-4cc14ff9dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is Parallel sparse triangular solve?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5344dbd-36b4-46ca-bc1b-403bce4e9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"is there any CUDA code in your dataset?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4bf5b-3d8b-4269-936d-afa376714e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"list the sample of CUDA code from your dataset?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce67d3-8cbf-4ace-b078-44cb92f4b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is execution model in python?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5853b-7b69-4eed-b9fe-36e4769bf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"write a CUDA code snippet for adding 2 numbers\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab717e-960a-4c7d-b58b-b3aa16c39516",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is execution model in python?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d964c-9687-4289-85d4-a4e718d40647",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"__global__ void permute(int n, int *data) {\\n extern__shared__ int smem[];\\n if (n <= 1)\\n return;\\n\\nsmem[threadIdx.x] = data[threadIdx.x];\\n __syncthreads();\\n\\n permute_data(smem, n);\\n __syncthreads();\\n\\n // Write back to GMEM since we can not pass SMEM to children.\\n data[threadIdx.x] = smem[threadIdx.x];\\n __syncthreads();\\n\\n if (threadIdx.x == 0) {\\n permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data);\\n permute<<< 1,256, n/2*sizeof(int) >>>(n/2, data+n/2);\\n }\\n}\\n\\nvoid host_launch(int*data) {\\n permute<<< 1, 256, 256*sizeof(int) >>>(256, data);\\n}\\n\\n What is __syncthreads function doing in this code? \\n what is the syncthreadsfunction doing in this code?\" \n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5cf32-7dc1-457d-a550-fb9b1069361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is execution model in python?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df39d59f-7753-4174-a11f-82bbeb1a1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does CUDA manage reference to user objects when a cudaGraphExec_t is destroyed without being sychronized?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5f1fd-049b-4cdb-9868-3aefc44466cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"What is 7+9?\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021248eb-7cef-463d-b8c9-260135ff966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"Consider the given functions: $$\\begin{array}{ccc} f(x) & = & 5x^2 - \\frac{1}{x}+ 3\\\\ g(x) & = & x^2-k \\end{array}$$If $f(2) - g(2) = 2$, what is the value of $k$?\")\n",
    "\n",
    "print(out.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e263c66-4274-4c6f-ad98-b87ae14242db",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"Write a Cuda code for checking given string is plindrome or not\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f9c9f8-0d7c-4374-9852-576d6b85bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = llm.invoke(\"__global__ void permute(int n, int *data) {\\n extern__shared__ int smem[];\\n if (n <= 1)\\n return;\\n\\nsmem[threadIdx.x] = data[threadIdx.x];\\n __syncthreads();\\n\\n permute_data(smem, n);\\n __syncthreads();\\n\\n // Write back to GMEM since we can not pass SMEM to children.\\n data[threadIdx.x] = smem[threadIdx.x];\\n __syncthreads();\\n\\n if (threadIdx.x == 0) {\\n permute<<< 1, 256, n/2*sizeof(int) >>>(n/2, data);\\n permute<<< 1,256, n/2*sizeof(int) >>>(n/2, data+n/2);\\n }\\n}\\n\\nvoid host_launch(int*data) {\\n permute<<< 1, 256, 256*sizeof(int) >>>(256, data);\\n}\\n\\n What is __syncthreads function doing in this code? \\n what is the syncthreadsfunction doing in this code? \")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f4f76-c2d7-4b06-af60-0fb4f282dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker container stop llm_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d521e71-76bc-44c2-a1a9-49bea52d7c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
